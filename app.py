import streamlit as st
from dotenv import load_dotenv
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# .envèª­ã¿è¾¼ã¿
load_dotenv()

# OpenAI APIã‚­ãƒ¼
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ãƒ¢ãƒ‡ãƒ«æº–å‚™
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, openai_api_key=OPENAI_API_KEY)

# å°‚é–€å®¶ã®ç¨®é¡ï¼ˆé¸æŠè‚¢ï¼‰
experts = {
    "æ­´å²å­¦è€…": "ã‚ãªãŸã¯æ­´å²ã®å°‚é–€å®¶ã§ã™ã€‚æ­´å²çš„èƒŒæ™¯ã‚„å‡ºæ¥äº‹ã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    "åŒ»å¸«": "ã‚ãªãŸã¯åŒ»ç™‚ã®å°‚é–€å®¶ã§ã™ã€‚å¥åº·ã‚„ç—…æ°—ã«ã¤ã„ã¦æ­£ç¢ºã‹ã¤ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
    "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢": "ã‚ãªãŸã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚æŠ€è¡“çš„ãªèª²é¡Œã‚’è§£æ±ºã™ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚",
}

# LLMå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
def generate_answer(user_input: str, expert: str) -> str:
    template = ChatPromptTemplate.from_messages([
        ("system", experts[expert]),
        ("human", "{question}")
    ])
    chain = template | llm | StrOutputParser()
    return chain.invoke({"question": user_input})


# --- Streamlit UI ---
st.title("ğŸ¤– LangChain Ã— Streamlit (Lesson8é¢¨)")

st.write("""
### ä½¿ã„æ–¹
1. å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«è³ªå•ã‚„ç›¸è«‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„  
2. å°‚é–€å®¶ã‚’é¸ã‚“ã§ã€Œé€ä¿¡ã€ã‚’æŠ¼ã™ã¨ã€ãã®å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¾ã™  
""")

# ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
expert_choice = st.radio("å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„:", list(experts.keys()))

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
with st.form("form"):
    user_text = st.text_area("è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    submitted = st.form_submit_button("é€ä¿¡")

# å›ç­”è¡¨ç¤º
if submitted and user_text.strip():
    st.write("### å›ç­”")
    st.success(generate_answer(user_text, expert_choice))
